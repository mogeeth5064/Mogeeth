import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# -------------------- Page Setup --------------------
st.set_page_config(page_title="Energy Consumption Dashboard", layout="wide")
st.title("‚ö° Household Energy Consumption Prediction Dashboard")

# -------------------- Load Data --------------------
dataset_path = r"C:\Users\Mogeeth.M\Downloads\powerpluse\.venv\Scripts\predecteddata.csv"
try:
    df = pd.read_csv(dataset_path, parse_dates=['Datetime'])
except FileNotFoundError:
    st.error(f"Error: CSV file not found at {dataset_path}")
    st.stop()

st.subheader("Raw Data")
st.dataframe(df.head())

# -------------------- Data Preprocessing --------------------
st.subheader("üõ†Ô∏è Data Preprocessing")

# 1. Handle Missing Data
st.info("Handling Missing Data...")
df = df.dropna()  # Simple removal of rows with any missing values
st.info(f"Number of rows after handling missing data: {len(df)}")

# 2. Parse Date and Time Features
st.info("Parsing Date and Time Features...")
df['Year'] = df['Datetime'].dt.year
df['Month'] = df['Datetime'].dt.month
df['Day'] = df['Datetime'].dt.day
df['Hour'] = df['Datetime'].dt.hour
df['Minute'] = df['Datetime'].dt.minute
df['Second'] = df['Datetime'].dt.second
df['DayOfWeek'] = df['Datetime'].dt.dayofweek  # Monday=0, Sunday=6
st.dataframe(df[['Datetime', 'Year', 'Month', 'Day', 'Hour', 'Minute', 'Second', 'DayOfWeek']].head())

# 3. Create Additional Features
st.info("Creating Additional Features...")
df['Date'] = df['Datetime'].dt.date
daily_avg = df.groupby('Date')['Global_active_power'].mean().reset_index(name='Daily_Avg_Power')
df = pd.merge(df, daily_avg, on='Date', how='left')

# Define peak hours (you can customize these)
df['Is_Peak_Hour'] = df['Hour'].apply(lambda h: 1 if 17 <= h < 20 else 0) # Example: 5 PM to 8 PM

# Create rolling average (window size can be adjusted)
df['Rolling_Avg_Power_24h'] = df['Global_active_power'].rolling(window=24, min_periods=1).mean()
st.dataframe(df[['Datetime', 'Daily_Avg_Power', 'Is_Peak_Hour', 'Rolling_Avg_Power_24h']].head())

# Remove the temporary 'Date' column
df = df.drop('Date', axis=1)

# Display Processed Data Sample
st.subheader("Processed Data Sample")
st.dataframe(df.head())

# -------------------- Sidebar Filters (for EDA) --------------------
st.sidebar.markdown("---")
st.sidebar.subheader("üìÖ Filter by Year (for EDA)")
selected_year = st.sidebar.selectbox("Select Year", sorted(df['Year'].unique()))
filtered_df = df[df['Year'] == selected_year]

# -------------------- EDA: Energy Usage Over Time --------------------
st.subheader("üìä Energy Usage Over Time (on Processed Data)")
fig1, ax1 = plt.subplots(figsize=(14, 5))
ax1.plot(filtered_df['Datetime'], filtered_df['Global_active_power'], label='Power Consumption (kW)', color='orange')
ax1.set_title(f'Energy Usage in Year {selected_year}')
ax1.set_xlabel("Datetime")
ax1.set_ylabel("Global Active Power (kW)")
ax1.legend()
st.pyplot(fig1)

# -------------------- Model Training & Prediction --------------------
st.subheader("üîÆ Model Training & Prediction")

# User selects target and feature columns (excluding Datetime as it's now broken down)
numerical_cols = df.select_dtypes(include=np.number).columns.tolist()
date_time_cols = ['Year', 'Month', 'Day', 'Hour', 'Minute', 'Second', 'DayOfWeek']
available_cols = numerical_cols + date_time_cols
if 'Global_active_power' in available_cols:
    available_features = [col for col in available_cols if col != 'Global_active_power']
else:
    available_features = available_cols

target_column = st.selectbox("Select Target Column (the variable to predict):", df.columns)
feature_columns = st.multiselect("Select Feature Columns (the input variables):", [col for col in df.columns if col != target_column and col != 'Datetime'])

if target_column and feature_columns:
    st.info(f"Selected target column: {target_column}")
    st.info(f"Selected feature columns: {feature_columns}")

    if st.button("Train Model"):
        # Feature and Target Setup based on user selection
        target = target_column
        features = feature_columns
        X = df[features]
        y = df[target]

        if X.empty or y.empty:
            st.error("Error: Please select at least one feature column and a target column.")
        else:
            # 4. Normalize or Scale Numerical Data
            numerical_features = X.select_dtypes(include=np.number).columns
            if not numerical_features.empty:
                scaler = StandardScaler()
                X[numerical_features] = scaler.fit_transform(X[numerical_features])
                st.info("Numerical features scaled using StandardScaler.")
                st.dataframe(X[numerical_features].head())

            # Train/Test Split
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

            # Train Model
            with st.spinner("Training the model..."):
                model = RandomForestRegressor(n_estimators=100, random_state=42)
                try:
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)

                    # Model Metrics
                    st.markdown("### üìà Model Performance Metrics")
                    col1, col2, col3 = st.columns(3)
                    col1.metric("RMSE", f"{np.sqrt(mean_squared_error(y_test, y_pred)):.3f}")
                    col2.metric("MAE", f"{mean_absolute_error(y_test, y_pred):.3f}")
                    col3.metric("R¬≤ Score", f"{r2_score(y_test, y_pred):.3f}")

                    # -------------------- Feature Importance --------------------
                    st.subheader("üìå Feature Importance")
                    try:
                        importances = pd.Series(model.feature_importances_, index=features).sort_values(ascending=False)
                        fig2, ax2 = plt.subplots()
                        sns.barplot(x=importances.values[:10], y=importances.index[:10], ax=ax2, palette='viridis')
                        ax2.set_title("Top 10 Important Features")
                        st.pyplot(fig2)

                        # -------------------- Prediction Visualization --------------------
                        st.subheader("üß™ Actual vs Predicted Power Consumption")

                        # Create prediction comparison DataFrame
                        test_index = y_test.index
                        comparison_df = pd.DataFrame({
                            'Datetime': df.loc[test_index, 'Datetime'].values,
                            'Actual': y_test.values,
                            'Predicted': y_pred
                        }).sort_values('Datetime')

                        # Downsample for plotting performance
                        if 'Datetime' in comparison_df.columns:
                            comparison_df = comparison_df.set_index("Datetime").resample('H').mean().reset_index()

                            fig3, ax3 = plt.subplots(figsize=(14, 5))
                            ax3.plot(comparison_df['Datetime'], comparison_df['Actual'], label='Actual', color='blue')
                            ax3.plot(comparison_df['Datetime'], comparison_df['Predicted'], label='Predicted', alpha=0.7, color='red')
                            ax3.set_title("Actual vs Predicted Power Consumption")
                            ax3.set_xlabel("Datetime")
                            ax3.set_ylabel("Power (kW)")
                            ax3.legend()
                            st.pyplot(fig3)
                        else:
                            st.warning("Warning: 'Datetime' column not available for prediction visualization.")

                    except AttributeError:
                        st.warning("Warning: Feature importance not available for this model.")
                except Exception as e:
                    st.error(f"An error occurred during model training: {e}")
                    
